{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bda027a-bc73-4ab1-908b-e7fcfe89b51d",
   "metadata": {},
   "source": [
    "### Code de Dinh-Viet pour mieux comprendre QDN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25728367-bae9-4462-8344-e0945e98b192",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = HIVPatient(clipping=False, logscale=False)\n",
    "N_action = len(env.action_set) # 4\n",
    "DIM_state = len(env.state()) # 6\n",
    "patient.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af72bb4-5c1c-4372-aeff-5ad0b7da35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size = DIM_state, output_size = N_action):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(input_size,64)\n",
    "        self.l2 = nn.Linear(64,64)\n",
    "        self.l3 = nn.linear(64,output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ### batch_size = x.shape[0] ??\n",
    "        x1 = nn.Relu(self.l1(x))\n",
    "        x2 = nn.Relu(self.l2(x1))\n",
    "        output = self.l3(x2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6269251b-c916-4f73-a617-26f782b544c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_agent:\n",
    "    def __init__(self, config, model): ## Cf dictionnaire config plus bas\n",
    "        \n",
    "        self.gamma = config['gamma']\n",
    "        self.batch_size = config['batch_size'] ### Utile ??\n",
    "        self.nb_actions = config['nb_actions']\n",
    "        self.memory = ReplayBuffer(config['buffer_size'])\n",
    "        self.epsilon_max = config['epsilon_max']\n",
    "        self.epsilon_min = config['epsilon_min']\n",
    "        self.epsilon_stop = config['epsilon_decay_period']\n",
    "        self.epsilon_delay = config['epsilon_delay_decay']\n",
    "        self.epsilon_step = (self.epsilon_max - self.epsilon_min) / self.epsilon_stop\n",
    "        self.nb_gradient_steps = config['gradient_steps']\n",
    "        self.total_steps = 0\n",
    "        self.model = model \n",
    "        self.best_model = None\n",
    "        \n",
    "        self.target_model = copy.deepcopy(self.model).to(device) \n",
    "        self.update_target_freq = config['update_target_freq']\n",
    "        \n",
    "        self.criterion = torch.nn.MSELoss() # torch.nn.SmoothL1Loss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config['learning_rate'])\n",
    "        self.reset_every = config['reset_every']\n",
    "        self.plot = config['plot']\n",
    "        \n",
    "        self.target_model.eval()\n",
    "\n",
    "    def make_training_data(self):\n",
    "        \"\"\"Splits a sample of the buffer in multiple tensors\"\"\"\n",
    "        batch = self.memory.sample(self.batch_size)\n",
    "        X, A, R, Y, D = [], [], [], [], []\n",
    "        for sample in batch:\n",
    "            X.append(sample[0])\n",
    "            A.append(sample[1])\n",
    "            R.append(sample[2])\n",
    "            Y.append(sample[3])\n",
    "            D.append(sample[4])\n",
    "            \n",
    "        return torch.Tensor(X), torch.Tensor(A), torch.Tensor(R), torch.Tensor(Y), torch.Tensor(D)\n",
    "          \n",
    "        \n",
    "    def print_grads(self):\n",
    "        \"\"\"Displays the gradients max and min\"\"\"\n",
    "        print(\n",
    "            \"fc3 : [{:.2e}, {:.2e}] ; fc2 : [{:.2e}, {:.2e}] ; fc1 : [{:.2e}, {:.2e}]\".format(\n",
    "                torch.min(self.model.fc3.weight.grad).item(),\n",
    "                torch.max(self.model.fc3.weight.grad).item(),\n",
    "                torch.min(self.model.fc2.weight.grad).item(),\n",
    "                torch.max(self.model.fc2.weight.grad).item(),\n",
    "                torch.min(self.model.fc1.weight.grad).item(),\n",
    "                torch.max(self.model.fc1.weight.grad).item(),\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def gradient_step(self):\n",
    "        running_loss = 0\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            X, A, R, Y, D = self.make_training_data()\n",
    "            X, A, R, Y, D = X.to(device), A.to(device), R.to(device), Y.to(device), D.to(device)\n",
    "            QYmax = self.target_model(Y).max(1)[0].detach()\n",
    "            update = torch.addcmul(R, self.gamma, 1-D, QYmax) # update = R + gamma * (1-D) * QYmax\n",
    "            QXA = self.model(X).gather(1, A.to(torch.long).unsqueeze(1)) # Concatenate S (=X) & A\n",
    "            loss = self.criterion(QXA, update.unsqueeze(1)) # MSE Loss\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1)\n",
    "            self.optimizer.step() \n",
    "        \n",
    "        return running_loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train(self, env, max_episode):\n",
    "        episode_return = []\n",
    "        loss_return = []\n",
    "        episode = 0\n",
    "        episode_cum_reward = 0\n",
    "        state = env.reset()\n",
    "        epsilon = self.epsilon_max\n",
    "        step = 0\n",
    "        best_reward = 0\n",
    "        \n",
    "        \n",
    "        while episode < max_episode:\n",
    "            # update epsilon\n",
    "            if step > self.epsilon_delay:\n",
    "                epsilon = max(self.epsilon_min, epsilon-self.epsilon_step)\n",
    "#             epsilon = 0.15\n",
    "\n",
    "            # select epsilon-greedy action\n",
    "            action = choose_action(\n",
    "                torch.Tensor(state).unsqueeze(0).to(device), \n",
    "                self.model, EPS=epsilon)\n",
    "            \n",
    "            if isinstance(action, torch.Tensor): action = action.item()\n",
    "  \n",
    "            # step\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            new_item = (state, action, reward, next_state, done)\n",
    "            self.memory.append(new_item)\n",
    "            episode_cum_reward += reward\n",
    "\n",
    "            # train\n",
    "            running_loss = 0\n",
    "            for _ in range(self.nb_gradient_steps):\n",
    "                running_loss += self.gradient_step()\n",
    "                \n",
    "            \n",
    "            # update target\n",
    "            if step % self.update_target_freq == 0:\n",
    "                print(\"Updating target\")\n",
    "                self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "            # next transition\n",
    "            step += 1\n",
    "            \n",
    "            done = done or (step % self.reset_every == 0)\n",
    "            \n",
    "            # display stuff\n",
    "            if done:\n",
    "                episode += 1\n",
    "                print(\"Episode \", '{:3d}'.format(episode), \n",
    "                      \", step \", '{:6d}'.format(step), \n",
    "                      \", epsilon \", '{:6.2f}'.format(epsilon), \n",
    "                      \", memory size \", '{:5d}'.format(len(self.memory)), \n",
    "                      \", loss \", '{:.2e}'.format(running_loss), \n",
    "                      \", episode return \", '{:4.1f}'.format(episode_cum_reward),\n",
    "                      sep='')\n",
    "                \n",
    "                if len(self.memory) > self.batch_size: self.print_grads()\n",
    "                \n",
    "                if episode_cum_reward > best_reward:\n",
    "                    self.best_model = copy.deepcopy(self.model)\n",
    "                    best_reward = episode_cum_reward\n",
    "                    print(\"\\033[1m\\033[91m >>>>>> Best model update \\033[0m\\033[0m\")\n",
    "                    \n",
    "                state = env.reset()\n",
    "                episode_return.append(episode_cum_reward)\n",
    "                episode_cum_reward = 0\n",
    "                \n",
    "                print(\"===================================================\")\n",
    "                if self.plot:\n",
    "                    plot_reward(episode_return, 20)\n",
    "            else:\n",
    "                state = next_state\n",
    "\n",
    "        return episode_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb5cfa-b149-4e40-a42b-a231052f6e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'observation_space': DIM_STATE,\n",
    "          'nb_actions': N_ACTION,\n",
    "          'learning_rate': 0.001,\n",
    "          'gamma': 0.99,\n",
    "          'buffer_size': 60000,\n",
    "          'epsilon_max': 1.,\n",
    "          'epsilon_delay_decay': 20,\n",
    "          'update_target_freq': 80*2,\n",
    "       \n",
    "#          # --- HIV\n",
    "          'epsilon_min': 0.15,\n",
    "          'epsilon_decay_period': 2000,\n",
    "          'gradient_steps': 10,\n",
    "          'batch_size': 128,\n",
    "          'reset_every': 80,\n",
    "          'plot': False,\n",
    "         }\n",
    "\n",
    "\n",
    "agent = DQN_agent(config, DQN)\n",
    "scores = agent.train(env, 50)\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeea271d-8501-4f20-9c9c-2ea3b2ed86e9",
   "metadata": {},
   "source": [
    "## Results\n",
    "### Avec target, loss MSE, clip, hidden64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f24c63e-8cf5-4bf4-8751-d435551fc2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, IN_DIM=DIM_STATE, OUT_DIM=N_ACTION):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(IN_DIM, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, OUT_DIM)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "model = DenseNet()\n",
    "model.load_state_dict(torch.load('model_clip_hidden64.dqn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06053cc-1f3c-4857-bbb5-5dbc5083c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset('unhealthy') # 'uninfected', 'healthy'\n",
    "states = make_simulation_dqn(s, model)\n",
    "\n",
    "plot_stuff(states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
