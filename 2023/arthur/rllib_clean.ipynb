{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n",
      "device=device(type='mps')\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "from collections import deque, namedtuple\n",
    "from itertools import count\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ray\n",
    "import torch\n",
    "# import AlgorithmConfig\n",
    "from ray.rllib.algorithms.algorithm import AlgorithmConfig\n",
    "from ray.rllib.algorithms.dqn import DQN, DQNConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "sys.path.append('../..')\n",
    "\n",
    "from hiv_patient_gym import HIVPatientGym\n",
    "\n",
    "device: torch.device = torch.device(\"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"MPS is available\")\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"CUDA is not available\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"{device=}\")\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 12:02:34,857\tINFO worker.py:1553 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.10.9</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.3.0</b></td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.10.9', ray_version='2.3.0', ray_commit='cf7a56b4b0b648c324722df7c99c168e92ff0b45', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2023-03-21_12-02-32_816049_29773/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2023-03-21_12-02-32_816049_29773/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2023-03-21_12-02-32_816049_29773', 'metrics_export_port': 55009, 'gcs_address': '127.0.0.1:64377', 'address': '127.0.0.1:64377', 'dashboard_agent_listen_port': 52365, 'node_id': '8c68c88ae0fe8ecda5d07fd7c442594b68299d936411f6168cfdfd45'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init(num_gpus=1, num_cpus=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0\n",
    "\n",
    "config = (\n",
    "    DQNConfig(\n",
    "        {\n",
    "            \"num_workers\": 8,\n",
    "            \"num_gpus\": 1,\n",
    "            # \"num_envs_per_worker\": 1,\n",
    "            # \"num_cpus_per_worker\": 1,\n",
    "            # \"num_gpus_per_worker\": 0.125,\n",
    "            \"learning_starts\": 1000,\n",
    "            \"target_network_update_freq\": 100,\n",
    "            \"buffer_size\": 100_000,\n",
    "            \"gamma\": 0.98,\n",
    "            \"max_episode_len\": 200,\n",
    "            \"epsilon\": epsilon,  # add the epsilon value to the config\n",
    "            \"exploration_config\": {  # add the exploration config\n",
    "                \"type\": \"EpsilonGreedy\",\n",
    "                \"initial_epsilon\": epsilon,\n",
    "                \"final_epsilon\": 0.05,\n",
    "                \"epsilon_timesteps\": 5000,\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "    .framework(\"torch\")\n",
    "    .environment(HIVPatientGym)\n",
    "    # .torch_device(device)\n",
    ")\n",
    "\n",
    "\n",
    "# set torch backend to device\n",
    "config[\"torch_device\"] = device\n",
    "\n",
    "config.train_batch_size = 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 12:03:37,774\tINFO algorithm.py:506 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2023-03-21 12:03:37,779\tWARNING utils.py:161 -- `config.auto_wrap_old_gym_envs` is activated AND you seem to have provided an old gym-API environment. RLlib will therefore try to auto-fix the following error. However, please consider switching over to the new `gymnasium` APIs:\n",
      "Your environment ({}) does not abide to the new gymnasium-style API!\n",
      "From Ray 2.3 on, RLlib only supports the new (gym>=0.26 or gymnasium) Env APIs.\n",
      "{}\n",
      "Learn more about the most important changes here:\n",
      "https://github.com/openai/gym and here: https://github.com/Farama-Foundation/Gymnasium\n",
      "\n",
      "In order to fix this problem, do the following:\n",
      "\n",
      "1) Run `pip install gymnasium` on your command line.\n",
      "2) Change all your import statements in your code from\n",
      "   `import gym` -> `import gymnasium as gym` OR\n",
      "   `from gym.space import Discrete` -> `from gymnasium.spaces import Discrete`\n",
      "\n",
      "For your custom (single agent) gym.Env classes:\n",
      "3.1) Either wrap your old Env class via the provided `from gymnasium.wrappers import\n",
      "     EnvCompatibility` wrapper class.\n",
      "3.2) Alternatively to 3.1:\n",
      " - Change your `reset()` method to have the call signature 'def reset(self, *,\n",
      "   seed=None, options=None)'\n",
      " - Return an additional info dict (empty dict should be fine) from your `reset()`\n",
      "   method.\n",
      " - Return an additional `truncated` flag from your `step()` method (between `done` and\n",
      "   `info`). This flag should indicate, whether the episode was terminated prematurely\n",
      "   due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "For your custom RLlib `MultiAgentEnv` classes:\n",
      "4.1) Either wrap your old MultiAgentEnv via the provided\n",
      "     `from ray.rllib.env.wrappers.multi_agent_env_compatibility import\n",
      "     MultiAgentEnvCompatibility` wrapper class.\n",
      "4.2) Alternatively to 4.1:\n",
      " - Change your `reset()` method to have the call signature\n",
      "   'def reset(self, *, seed=None, options=None)'\n",
      " - Return an additional per-agent info dict (empty dict should be fine) from your\n",
      "   `reset()` method.\n",
      " - Rename `dones` into `terminateds` and only set this to True, if the episode is really\n",
      "   done (as opposed to has been terminated prematurely due to some horizon/time-limit\n",
      "   setting).\n",
      " - Return an additional `truncateds` per-agent dictionary flag from your `step()`\n",
      "   method, including the `__all__` key (100% analogous to your `dones/terminateds`\n",
      "   per-agent dict).\n",
      "   Return this new `truncateds` dict between `dones/terminateds` and `infos`. This\n",
      "   flag should indicate, whether the episode (for some agent or all agents) was\n",
      "   terminated prematurely due to some time constraint or other kind of horizon setting.\n",
      "\n",
      "2023-03-21 12:03:37,780\tWARNING env.py:156 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.\n",
      "2023-03-21 12:03:37,780\tWARNING env.py:166 -- Your env reset() method appears to take 'seed' or 'return_info' arguments. Note that these are not yet supported in RLlib. Seeding will take place using 'env.seed()' and the info dict will not be returned from reset.\n",
      "2023-03-21 12:03:38,069\tWARNING util.py:67 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 1000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0077533721923828125\n",
      "  StateBufferConnector_ms: 0.0028371810913085938\n",
      "  ViewRequirementAgentConnector_ms: 0.08810997009277344\n",
      "counters:\n",
      "  num_agent_steps_sampled: 1000\n",
      "  num_agent_steps_trained: 0\n",
      "  num_env_steps_sampled: 1000\n",
      "  num_env_steps_trained: 0\n",
      "custom_metrics: {}\n",
      "date: 2023-03-21_12-04-05\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 14064693.777192889\n",
      "episode_reward_mean: 9848365.56553675\n",
      "episode_reward_min: 6992912.613588632\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 5\n",
      "experiment_id: d7e3a0aa7fba4b8b8e934aa5994cccae\n",
      "hostname: MacBook-Air-de-Arthur.local\n",
      "info:\n",
      "  learner: {}\n",
      "  num_agent_steps_sampled: 1000\n",
      "  num_agent_steps_trained: 0\n",
      "  num_env_steps_sampled: 1000\n",
      "  num_env_steps_trained: 0\n",
      "iterations_since_restore: 1\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 1000\n",
      "num_agent_steps_trained: 0\n",
      "num_env_steps_sampled: 1000\n",
      "num_env_steps_sampled_this_iter: 1000\n",
      "num_env_steps_trained: 0\n",
      "num_env_steps_trained_this_iter: 0\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 0\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 0\n",
      "perf:\n",
      "  cpu_util_percent: 67.79230769230769\n",
      "  ram_util_percent: 72.88974358974357\n",
      "pid: 29773\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06535050871369841\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 25.29954291009284\n",
      "  mean_inference_ms: 0.7906360226077633\n",
      "  mean_raw_obs_processing_ms: 0.4392580076173827\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0077533721923828125\n",
      "    StateBufferConnector_ms: 0.0028371810913085938\n",
      "    ViewRequirementAgentConnector_ms: 0.08810997009277344\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 14064693.777192889\n",
      "  episode_reward_mean: 9848365.56553675\n",
      "  episode_reward_min: 6992912.613588632\n",
      "  episodes_this_iter: 5\n",
      "  hist_stats:\n",
      "    episode_lengths: [200, 200, 200, 200, 200]\n",
      "    episode_reward: [8484121.113730296, 10147524.940038888, 6992912.613588632, 14064693.777192889,\n",
      "      9552575.383133046]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06535050871369841\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.29954291009284\n",
      "    mean_inference_ms: 0.7906360226077633\n",
      "    mean_raw_obs_processing_ms: 0.4392580076173827\n",
      "time_since_restore: 27.054342031478882\n",
      "time_this_iter_s: 27.054342031478882\n",
      "time_total_s: 27.054342031478882\n",
      "timers:\n",
      "  training_iteration_time_ms: 28.29\n",
      "timestamp: 1679396645\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 1000\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "warmup_time: 0.2989521026611328\n",
      "\n",
      "checkpoint saved at /Users/arthur/ray_results/DQN_HIVPatientGym_2023-03-21_12-03-37xk71p8z5/checkpoint_000001\n",
      "agent_timesteps_total: 2000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.00843048095703125\n",
      "  StateBufferConnector_ms: 0.003211498260498047\n",
      "  ViewRequirementAgentConnector_ms: 0.08907079696655273\n",
      "counters:\n",
      "  last_target_update_ts: 1501\n",
      "  num_agent_steps_sampled: 2000\n",
      "  num_agent_steps_trained: 50000\n",
      "  num_env_steps_sampled: 2000\n",
      "  num_env_steps_trained: 50000\n",
      "  num_target_updates: 2\n",
      "custom_metrics: {}\n",
      "date: 2023-03-21_12-06-55\n",
      "done: false\n",
      "episode_len_mean: 200.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 16542791.4466468\n",
      "episode_reward_mean: 10017806.26867135\n",
      "episode_reward_min: 6992912.613588632\n",
      "episodes_this_iter: 5\n",
      "episodes_total: 10\n",
      "experiment_id: d7e3a0aa7fba4b8b8e934aa5994cccae\n",
      "hostname: MacBook-Air-de-Arthur.local\n",
      "info:\n",
      "  last_target_update_ts: 1501\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 898.88\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 40.0\n",
      "        max_q: 14685.7529296875\n",
      "        mean_q: 12404.96484375\n",
      "        min_q: 6845.029296875\n",
      "      mean_td_error: -56893.16015625\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 50.0\n",
      "      num_grad_updates_lifetime: 1000.0\n",
      "      td_error: [-41328.25, -23225.140625, -87905.875, -45390.96875, -21153.48046875,\n",
      "        -85467.5625, -47946.71875, -122682.171875, -78593.625, -32565.25, -17698.8515625,\n",
      "        -65395.02734375, -52282.25390625, -39130.49609375, -42645.4453125, -109351.5859375,\n",
      "        -39130.49609375, -26123.73046875, -39130.49609375, -78593.625, -113991.2890625,\n",
      "        -58407.1171875, -84937.2734375, -14433.84765625, -46617.81640625, -46298.9453125,\n",
      "        -32936.3359375, -7359.6875, -72932.328125, -53085.328125, -57215.15625, -33565.5546875,\n",
      "        -96896.8359375, -9218.6416015625, -58407.1171875, -70824.4140625, -46320.9609375,\n",
      "        -132603.03125, -30571.16796875, -48243.20703125, -22437.732421875, -101606.90625,\n",
      "        -33565.5546875, -87733.4765625, -113991.2890625, -53419.05859375, -46617.81640625,\n",
      "        -81819.671875, -54150.36328125, -38709.07421875]\n",
      "  num_agent_steps_sampled: 2000\n",
      "  num_agent_steps_trained: 50000\n",
      "  num_env_steps_sampled: 2000\n",
      "  num_env_steps_trained: 50000\n",
      "  num_target_updates: 2\n",
      "iterations_since_restore: 2\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 2000\n",
      "num_agent_steps_trained: 50000\n",
      "num_env_steps_sampled: 2000\n",
      "num_env_steps_sampled_this_iter: 1000\n",
      "num_env_steps_trained: 50000\n",
      "num_env_steps_trained_this_iter: 50000\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 0\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 50000\n",
      "perf:\n",
      "  cpu_util_percent: 92.3701244813278\n",
      "  ram_util_percent: 72.82199170124481\n",
      "pid: 29773\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.06662255701929971\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 25.153201977886958\n",
      "  mean_inference_ms: 0.7980728811986462\n",
      "  mean_raw_obs_processing_ms: 0.45205381608745787\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.00843048095703125\n",
      "    StateBufferConnector_ms: 0.003211498260498047\n",
      "    ViewRequirementAgentConnector_ms: 0.08907079696655273\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 200.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 16542791.4466468\n",
      "  episode_reward_mean: 10017806.26867135\n",
      "  episode_reward_min: 6992912.613588632\n",
      "  episodes_this_iter: 5\n",
      "  hist_stats:\n",
      "    episode_lengths: [200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\n",
      "    episode_reward: [8484121.113730296, 10147524.940038888, 6992912.613588632, 14064693.777192889,\n",
      "      9552575.383133046, 7821168.142682562, 16542791.4466468, 10115167.285237903,\n",
      "      8662206.168088606, 7794901.8163738875]\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06662255701929971\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 25.153201977886958\n",
      "    mean_inference_ms: 0.7980728811986462\n",
      "    mean_raw_obs_processing_ms: 0.45205381608745787\n",
      "time_since_restore: 197.29700112342834\n",
      "time_this_iter_s: 170.24265909194946\n",
      "time_total_s: 197.29700112342834\n",
      "timers:\n",
      "  learn_throughput: 300.18\n",
      "  learn_time_ms: 166.567\n",
      "  load_throughput: 352284.898\n",
      "  load_time_ms: 0.142\n",
      "  synch_weights_time_ms: 0.025\n",
      "  training_iteration_time_ms: 203.63\n",
      "timestamp: 1679396815\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 2000\n",
      "training_iteration: 2\n",
      "trial_id: default\n",
      "warmup_time: 0.2989521026611328\n",
      "\n",
      "checkpoint saved at /Users/arthur/ray_results/DQN_HIVPatientGym_2023-03-21_12-03-37xk71p8z5/checkpoint_000002\n"
     ]
    }
   ],
   "source": [
    "algo = DQN(config=config)\n",
    "\n",
    "results = []\n",
    "best_rewards = [0]\n",
    "\n",
    "exploration_states = []\n",
    "\n",
    "for i in range(40):\n",
    "    # patient.reset(mode=\"unhealthy\")\n",
    "    \n",
    "    result = algo.train()\n",
    "    results.append(result)\n",
    "\n",
    "    best_rewards.append(result[\"episode_reward_max\"])\n",
    "\n",
    "    policy = algo.get_policy()\n",
    "    exploration_states.append(policy.exploration.get_state())\n",
    "\n",
    "    print(pretty_print(result))\n",
    "\n",
    "    if best_rewards[-1] > best_rewards[-2] or i % 10 == 0:\n",
    "        checkpoint = algo.save()\n",
    "        print(\"checkpoint saved at\", checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = HIVPatientGym()\n",
    "\n",
    "policy = algo.get_policy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "state = patient.reset(mode=\"unhealthy\")\n",
    "# state = patient.reset(mode=\"healthy\")\n",
    "\n",
    "total_reward= 0\n",
    "\n",
    "actions=[]\n",
    "\n",
    "for i in range(200):\n",
    "    action, *_ = policy.compute_single_action(state, explore=False)\n",
    "    actions.append(action)\n",
    "\n",
    "    state, reward, *_ = patient.step(action)\n",
    "    states.append(state)\n",
    "\n",
    "    total_reward += reward\n",
    "\n",
    "print(total_reward * 1e-6)\n",
    "\n",
    "states_null = []\n",
    "state = patient.reset(mode=\"unhealthy\")\n",
    "total_reward_null = 0\n",
    "\n",
    "# action = patient.action_set[0]\n",
    "action = 3\n",
    "for i in range(200):\n",
    "    # action = greedy_action(DQN, state)\n",
    "    state, reward, *_ = patient.step(action)\n",
    "    states_null.append(state)\n",
    "    total_reward_null += reward\n",
    "\n",
    "print(total_reward_null * 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(10, 7))\n",
    "npst = np.array(states)\n",
    "axs[0,0].plot(npst[:,0])\n",
    "axs[0,0].set_title(\"T1\")\n",
    "axs[0,1].plot(npst[:,1])\n",
    "axs[0,1].set_title(\"T1*\")\n",
    "axs[0,2].plot(npst[:,2])\n",
    "axs[0,2].set_title(\"T2\")\n",
    "axs[1,0].plot(npst[:,3])\n",
    "axs[1,0].set_title(\"T2*\")\n",
    "axs[1,1].plot(npst[:,4])\n",
    "axs[1,1].set_title(\"V\")\n",
    "axs[1,2].plot(npst[:,5])\n",
    "axs[1,2].set_title(\"E\")\n",
    "\n",
    "npst = np.array(states_null)\n",
    "axs[0,0].plot(npst[:,0])\n",
    "axs[0,1].plot(npst[:,1])\n",
    "axs[0,2].plot(npst[:,2])\n",
    "axs[1,0].plot(npst[:,3])\n",
    "axs[1,1].plot(npst[:,4])\n",
    "axs[1,2].plot(npst[:,5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(actions)), actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-in-depth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
